{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e70f3e3d-b9c9-4cf9-aa5d-bde765d58c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kd2939/dev/miniconda3/envs/emo/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "\n",
    "# os.environ['TORCH_LOGS']=\"+dynamo\"\n",
    "# os.environ['TORCHDYNAMO_VERBOSE']=\"1\"\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchaudio\n",
    "import torchaudio.functional as Fa\n",
    "from pytorch_metric_learning import losses\n",
    "import torchmetrics\n",
    "import webdataset as wds\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import get_token\n",
    "from accelerate import Accelerator\n",
    "from transformers import WhisperConfig, WhisperModel, WhisperFeatureExtractor\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from audiomentations import (\n",
    "    Compose,\n",
    "    AddGaussianNoise,\n",
    "    TimeStretch,\n",
    "    PitchShift,\n",
    "    PolarityInversion,\n",
    "    AdjustDuration,\n",
    "    Normalize,\n",
    ")\n",
    "from itertools import chain\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8c1de8e-f2f9-46e8-bf06-5630504004f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "# from huggingface_hub import HfApi\n",
    "# hf = HfApi()\n",
    "\n",
    "# hf.snapshot_download(\n",
    "#     \"columbiaslp/MSP_PODCAST\",\n",
    "#     repo_type=\"dataset\",\n",
    "#     local_dir_use_symlinks=False,\n",
    "#     local_dir=\"./data/MSP_PODCAST\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9172f005-19fd-4da8-9695-a1e3dccc85bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_url = \"./data/MSP_PODCAST/data/train/train_{01..21}-of-21.tar\"\n",
    "dev_url = \"./data/MSP_PODCAST/data/development/development_{01..05}-of-05.tar\"\n",
    "test_url = \"./data/MSP_PODCAST/data/test1/test1_{01..08}-of-08.tar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4c27085-2ba3-41bc-88a6-8864b943be0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 16_000\n",
    "max_duration = 6  # has to be int and such that 3000 / (max_duration / 30) is an int\n",
    "assert 3000 * (max_duration / 30 / 2) % 1 == 0\n",
    "crop_duration = 3\n",
    "shift_offset = 1\n",
    "feature_extractor = WhisperFeatureExtractor(chunk_length=max_duration)\n",
    "batch_size = 128\n",
    "epoch_length = 500\n",
    "dataloader_workers = 32\n",
    "augmentation = Compose(\n",
    "    [\n",
    "        AdjustDuration(duration_seconds=crop_duration, padding_mode=\"reflect\", p=0.5),\n",
    "        PolarityInversion(p=0.5),\n",
    "        Normalize(p=1.0),\n",
    "        AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=1.0),\n",
    "        TimeStretch(min_rate=0.8, max_rate=1.2, leave_length_unchanged=True, p=0.5),\n",
    "        PitchShift(min_semitones=-1, max_semitones=1, p=0.5),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7cb4cc8-834e-417c-bc50-fffb17de0a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import (\n",
    "    collation_fn,\n",
    "    decode,\n",
    "    crop,\n",
    "    apply_augmentation,\n",
    "    MSP_PODCAST_EMOTION_TO_IX,\n",
    "    MSP_PODCAST_EMOTIONS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2a532b9-5d3b-4157-9160-70bb81e9c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the basic WebDataset definition: it starts with a URL and add shuffling,\n",
    "# decoding, and augmentation. Note `resampled=True`; this is essential for\n",
    "# distributed training to work correctly.\n",
    "trainset = (\n",
    "    wds.WebDataset(train_url, shardshuffle=True, resampled=True)\n",
    "    .shuffle(8192)\n",
    "    .map(decode())\n",
    "    .map(apply_augmentation(augmentation=augmentation))\n",
    "    .map(crop(crop_duration=max_duration, random=True))\n",
    "    .batched(\n",
    "        batchsize=batch_size,\n",
    "        collation_fn=collation_fn(feature_extractor=feature_extractor),\n",
    "        partial=False,\n",
    "    )\n",
    ")\n",
    "trainloader = wds.WebLoader(\n",
    "    trainset,\n",
    "    batch_size=None,\n",
    "    num_workers=dataloader_workers,\n",
    "    prefetch_factor=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "# A resampled dataset is infinite size, but we can recreate a fixed epoch length.\n",
    "trainloader = trainloader.with_epoch(epoch_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdf239eb-1e19-496d-a912-eed517fbbca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "devset = (\n",
    "    wds.WebDataset(dev_url, shardshuffle=False, resampled=False)\n",
    "    .map(decode())\n",
    "    .map(crop(crop_duration=max_duration, random=False))\n",
    "    .batched(\n",
    "        batchsize=batch_size,\n",
    "        collation_fn=collation_fn(feature_extractor=feature_extractor),\n",
    "        partial=False,\n",
    "    )\n",
    ")\n",
    "devloader = wds.WebLoader(\n",
    "    devset,\n",
    "    batch_size=None,\n",
    "    num_workers=dataloader_workers,\n",
    "    prefetch_factor=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5223fd7-d880-49d6-be3c-58808c622a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(trainset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "342f4293-bcd5-4d09-8a0e-89a2cf44b731",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import WhisperBackbone, Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2826c158-504c-4b58-8b7f-3f4b2d028511",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperBackbone(\n",
    "    \"openai/whisper-tiny.en\", max_duration=max_duration, pooling=\"mean\"\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3ff91a1-c697-4494-aa73-a5013710e797",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Classifier(\n",
    "    \"openai/whisper-tiny.en\",\n",
    "    max_duration=max_duration,\n",
    "    projection_dim=128,\n",
    "    num_classes=len(MSP_PODCAST_EMOTION_TO_IX),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3b82c44f-9e73-45e6-b689-5caeaba50f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(whisper_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4a93edf-be47-4292-9399-191bb32d5216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    out = model(batch[\"feats\"][\"orig\"].cuda(), batch[\"attention_mask\"][\"orig\"].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "911f89c8-ef77-4f46-b638-6fc355f4fad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 384])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.matshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0dcfe06-ac2b-4c56-a567-42aad9dad0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [00:22,  3.36it/s]\n"
     ]
    }
   ],
   "source": [
    "emotions = []\n",
    "pre_contrastive_embeddings = []\n",
    "for batch in tqdm(devloader):\n",
    "    with torch.inference_mode():\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            embeddings = model(\n",
    "                batch[\"feats\"][\"orig\"].cuda(), batch[\"attention_mask\"][\"orig\"].cuda()\n",
    "            )\n",
    "        emotions.append(batch[\"emotion_ix\"])\n",
    "        pre_contrastive_embeddings.append(embeddings.cpu())\n",
    "pre_contrastive_embeddings = torch.cat(pre_contrastive_embeddings).numpy()\n",
    "emotions = torch.cat(emotions).numpy()\n",
    "pre_contrastive_tsne = TSNE(n_jobs=32).fit_transform(pre_contrastive_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79b6b3e9-6d8d-424c-82ed-538703801682",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\n",
    "    \"neutral\": \"gray\",\n",
    "    \"happy\": \"yellow\",\n",
    "    \"sad\": \"blue\",\n",
    "    \"angry\": \"red\",\n",
    "    \"fear\": \"purple\",\n",
    "    \"disgust\": \"darkgreen\",\n",
    "    \"surprise\": \"pink\",\n",
    "    \"contempt\": \"brown\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "774dafac-3d7f-4a92-9cf1-a5266d9e5653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne(title, embeddings):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.set_title(title)\n",
    "    for emotion in [\n",
    "        \"neutral\",\n",
    "        \"happy\",\n",
    "        \"sad\",\n",
    "        \"angry\",\n",
    "        \"fear\",\n",
    "        \"disgust\",\n",
    "        \"surprise\",\n",
    "        \"contempt\",\n",
    "    ]:\n",
    "        y = embeddings[emotions == MSP_PODCAST_EMOTION_TO_IX[emotion]]\n",
    "        ax.scatter(y[:, 0], y[:, 1], alpha=0.6, s=2, c=colors[emotion], label=emotion)\n",
    "    legend = ax.legend()\n",
    "    for handle in legend.legend_handles:\n",
    "        handle._sizes = [30]\n",
    "    plt.close()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d518d40-4731-47b6-a11c-0add213e1cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e23aecf3-292a-4c08-8249-d923534381f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conrastive_loss_func = losses.SelfSupervisedLoss(losses.NTXentLoss())\n",
    "optimizer = torch.optim.AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb156e05-77d6-42bd-bb2c-d72c4c8d4b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.59: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [03:23<00:00,  2.45it/s]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    with tqdm(total=500) as pbar:\n",
    "        tot_train_loss = 0\n",
    "        for i, batch in enumerate(trainloader):\n",
    "            optimizer.zero_grad()\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                embeddings_orig = model(\n",
    "                    batch[\"feats\"][\"orig\"].cuda(),\n",
    "                    batch[\"attention_mask\"][\"orig\"].cuda(),\n",
    "                )\n",
    "                embeddings_aug = model(\n",
    "                    batch[\"feats\"][\"aug\"].cuda(), batch[\"attention_mask\"][\"aug\"].cuda()\n",
    "                )\n",
    "                loss = conrastive_loss_func(embeddings_orig, embeddings_aug)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tot_train_loss += loss.detach()\n",
    "            pbar.update(1)\n",
    "            if i % 10 == 0:\n",
    "                pbar.set_description(f\"loss: {tot_train_loss / i:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7373a891-a381-457d-8978-c9a2ae697f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [00:22,  3.36it/s]\n"
     ]
    }
   ],
   "source": [
    "post_contrastive_embeddings = []\n",
    "for batch in tqdm(devloader):\n",
    "    with torch.inference_mode():\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            embeddings = model(\n",
    "                batch[\"feats\"][\"orig\"].cuda(), batch[\"attention_mask\"][\"orig\"].cuda()\n",
    "            )\n",
    "        post_contrastive_embeddings.append(embeddings.cpu())\n",
    "post_contrastive_embeddings = torch.cat(post_contrastive_embeddings).numpy()\n",
    "post_contrastive_tsne = TSNE(n_jobs=32).fit_transform(post_contrastive_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74c53071-88ab-4312-9488-1f5f4a812610",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()\n",
    "contrastive_weights = model.state_dict()\n",
    "model.load_state_dict(whisper_weights)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fdcc112d-2f6c-4530-a670-00bffa5a7a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_contrastive_loss_func = losses.NTXentLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3854fab5-1194-4692-a8af-63af0aba66cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 4.51: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [01:12<00:00,  6.94it/s]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    with tqdm(total=500) as pbar:\n",
    "        tot_train_loss = 0\n",
    "        for i, batch in enumerate(trainloader):\n",
    "            optimizer.zero_grad()\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                embeddings_orig = model(\n",
    "                    batch[\"feats\"][\"orig\"].cuda(),\n",
    "                    batch[\"attention_mask\"][\"orig\"].cuda(),\n",
    "                )\n",
    "                loss = supervised_contrastive_loss_func(\n",
    "                    embeddings_orig, labels=batch[\"emotion_ix\"].cuda()\n",
    "                )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tot_train_loss += loss.detach()\n",
    "            pbar.update(1)\n",
    "            if i % 10 == 0:\n",
    "                pbar.set_description(f\"loss: {tot_train_loss / i:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "324ded69-5f6d-48a5-822e-55472287e1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [00:23,  3.34it/s]\n"
     ]
    }
   ],
   "source": [
    "post_supervised_contrastive_embeddings = []\n",
    "for batch in tqdm(devloader):\n",
    "    with torch.inference_mode():\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            embeddings = model(\n",
    "                batch[\"feats\"][\"orig\"].cuda(), batch[\"attention_mask\"][\"orig\"].cuda()\n",
    "            )\n",
    "        post_supervised_contrastive_embeddings.append(embeddings.cpu())\n",
    "post_supervised_contrastive_embeddings = torch.cat(\n",
    "    post_supervised_contrastive_embeddings\n",
    ").numpy()\n",
    "post_supervised_contrastive_tsne = TSNE(n_jobs=32).fit_transform(\n",
    "    post_supervised_contrastive_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4ec12700-6b5e-488f-9090-e6ac5d5feddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()\n",
    "supervised_contrastive_weights = model.state_dict()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c2b9f921-cb75-42b5-8dab-6de452e303eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_tsne(\n",
    "    \"TSNE on embeddings pre-contrastive (Whisper weights)\",\n",
    "    embeddings=pre_contrastive_tsne,\n",
    ")\n",
    "fig.savefig(\"Whisper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "4c8c77bf-fa5c-4663-992f-cd1e37b11e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_tsne(\n",
    "    \"TSNE on embeddings post-contrastive (new weights)\",\n",
    "    embeddings=post_contrastive_tsne,\n",
    ")\n",
    "fig.savefig(\"Contrastive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "e1925cf5-1761-430b-bac5-148626a807dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_tsne(\n",
    "    \"TSNE on embeddings post-supervised-contrastive (using labels)\",\n",
    "    embeddings=post_supervised_contrastive_tsne,\n",
    ")\n",
    "fig.savefig(\"Supervised Contrastive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "659502dd-d2cb-4161-9c9a-db17c4fe13db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(contrastive_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "633809c8-6b11-48c0-bb86-de6500fd1319",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, pretrained_model_name_or_path, max_duration):\n",
    "        super().__init__()\n",
    "        model = WhisperModel.from_pretrained(pretrained_model_name_or_path)\n",
    "        state_dict = model.state_dict()\n",
    "        offset = int(3000 * (max_duration / 30 / 2))\n",
    "        state_dict[\"encoder.embed_positions.weight\"] = state_dict[\n",
    "            \"encoder.embed_positions.weight\"\n",
    "        ][:offset, :]\n",
    "        config = WhisperConfig.from_pretrained(\n",
    "            pretrained_model_name_or_path, max_source_positions=offset\n",
    "        )\n",
    "        model = WhisperModel(config)\n",
    "        model.load_state_dict(state_dict)\n",
    "        self.encoder = model.get_encoder()\n",
    "        self.projection = nn.Linear(in_features=384, out_features=128)\n",
    "        self.classification_head = nn.Linear(\n",
    "            in_features=128, out_features=len(MSP_PODCAST_EMOTION_TO_IX)\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, input_values: torch.Tensor, attention_mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        out = self.encoder(input_values).last_hidden_state\n",
    "        output_attention_mask = attention_mask[\n",
    "            :, ::2, None\n",
    "        ]  # second conv has stride of 2, so drop half\n",
    "        out = out * output_attention_mask\n",
    "        out = out.sum(dim=1, keepdim=True) / output_attention_mask.sum(\n",
    "            dim=1, keepdim=True\n",
    "        )\n",
    "        out = out.squeeze()\n",
    "        out = self.projection(out)\n",
    "        out = F.gelu(out)\n",
    "        out = self.classification_head(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28efe650-2541-43a7-8228-78f36f866c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "98df231c-0585-42a5-b00b-d4a1eca22866",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(\"openai/whisper-tiny.en\", max_duration=max_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "377daab0-4f8c-41c7-8304-8ca55f7124a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['projection.weight', 'projection.bias', 'classification_head.weight', 'classification_head.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(contrastive_weights, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b65ae8d2-5b1e-42e9-9f50-2a52e94a2a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()\n",
    "# for params in model.encoder.parameters():\n",
    "#     params.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ba7aa96a-9e05-471d-9e77-77ef8b787e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "1ea4228c-07df-48c1-92cf-48ee08e43aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    preds = model(\n",
    "        batch[\"feats\"][\"orig\"].cuda(),\n",
    "        batch[\"attention_mask\"][\"orig\"].cuda(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b097b67b-a388-4aad-8237-db9796b372da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.76: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [02:26<00:00,  3.41it/s]\n",
      "loss: 1.73: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [02:31<00:00,  3.29it/s]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    with tqdm(total=500) as pbar:\n",
    "        tot_train_loss = 0\n",
    "        for i, batch in enumerate(trainloader):\n",
    "            optimizer.zero_grad()\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                preds = model(\n",
    "                    batch[\"feats\"][\"orig\"].cuda(),\n",
    "                    batch[\"attention_mask\"][\"orig\"].cuda(),\n",
    "                )\n",
    "                loss = cross_entropy_loss_func(preds, batch[\"emotion_ix\"].cuda())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tot_train_loss += loss.detach()\n",
    "            pbar.update(1)\n",
    "            if i % 10 == 0:\n",
    "                pbar.set_description(f\"loss: {tot_train_loss / i:.2f}\")\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecc35ce-2c9a-49a1-a349-9558a8fd45c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "truth = []\n",
    "for batch in tqdm(devloader):\n",
    "    with torch.inference_mode():\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            preds = (\n",
    "                model(\n",
    "                    batch[\"feats\"][\"orig\"].cuda(),\n",
    "                    batch[\"attention_mask\"][\"orig\"].cuda(),\n",
    "                )\n",
    "                .argmax(-1)\n",
    "                .cpu()\n",
    "            )\n",
    "        predictions.append(preds)\n",
    "        truth.append(batch[\"emotion_ix\"])\n",
    "\n",
    "predictions = torch.cat(predictions).numpy()\n",
    "truth = torch.cat(truth).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f56a6a2-41cb-4cf2-9076-3cee9c9bf48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(truth, predictions, target_names=MSP_PODCAST_EMOTIONS))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
