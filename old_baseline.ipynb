{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "175f7c46-67ec-4386-a1d9-e5b7ef86b7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kd2939/dev/miniconda3/envs/emo/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "import time\n",
    "import gc\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "import IPython.display as ipd\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "# import torch.multiprocessing as mp\n",
    "\n",
    "# mp.set_start_method(\"spawn\", force=True)\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchaudio\n",
    "import torchmetrics\n",
    "import webdataset as wds\n",
    "from braceexpand import braceexpand\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import WhisperFeatureExtractor"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5379e693-6e1c-4758-a776-93817e131260",
   "metadata": {},
   "source": [
    "              precision    recall  f1-score   support\n",
    "\n",
    "     neutral       0.58      0.70      0.64        20\n",
    "        calm       0.65      0.88      0.74        40\n",
    "       happy       0.59      0.50      0.54        40\n",
    "         sad       0.48      0.38      0.42        40\n",
    "       angry       0.86      0.75      0.80        40\n",
    "     fearful       0.79      0.38      0.51        40\n",
    "     disgust       0.66      0.82      0.73        40\n",
    "   surprised       0.70      0.93      0.80        40\n",
    "\n",
    "    accuracy                           0.66       300\n",
    "   macro avg       0.66      0.67      0.65       300\n",
    "weighted avg       0.67      0.66      0.65       300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bc3cc8f-ae02-45af-867b-abd180df2c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def release_gpu_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcf7d4b2-8515-45dd-975f-00741fb0f895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_urls = list(braceexpand(\"./data/IEMOCAP_audio/data/Session{2..5}.tar\"))\n",
    "# dev_urls = list(braceexpand(\"./data/IEMOCAP_audio/data/Session1.tar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3022e55-8dd6-4ad6-9117-58d0ace55d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_urls = list(braceexpand(\"./data/RAVDESS_audio/data/Actor_{01..19}.tar\"))\n",
    "# dev_urls = list(braceexpand(\"./data/RAVDESS_audio/data/Actor_{20..24}.tar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fb74d90-080d-4363-8b5a-166e9346499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_urls = list(braceexpand(\"./data/MSP_PODCAST/data/train/train_{01..21}-of-21.tar\"))\n",
    "dev_urls = list(\n",
    "    braceexpand(\"./data/MSP_PODCAST/data/development/development_{01..05}-of-05.tar\")\n",
    ")\n",
    "test_urls = list(braceexpand(\"./data/MSP_PODCAST/data/test1/test1_{01..08}-of-08.tar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c91a55b8-a35d-424f-8317-bac4259b4c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import (\n",
    "    apply_augmentation,\n",
    "    decode,\n",
    "    crop,\n",
    "    collate_and_featurize,\n",
    "    MSP_PODCAST_EMOTIONS,\n",
    "    IEMOCAP_EMOTION_TO_IX,\n",
    "    IEMOCAP_EMOTIONS,\n",
    "    RAVDESS_EMOTIONS,\n",
    "    RAVDESS_EMOTION_TO_IX,\n",
    "    Cache,\n",
    "    FilterLabels,\n",
    "    MSP_PODCAST_EMOTION_TO_IX,\n",
    "    SampledShards,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "329181a8-17b0-4278-bc71-c4859edbf099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neutral',\n",
       " 'happy',\n",
       " 'sad',\n",
       " 'angry',\n",
       " 'fear',\n",
       " 'disgust',\n",
       " 'surprise',\n",
       " 'contempt',\n",
       " 'other',\n",
       " 'no_agreement']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSP_PODCAST_EMOTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c1201d2-e423-47e8-a67a-dfe689b26ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# counter = Counter()\n",
    "# for batch in tqdm(dl):\n",
    "#     counter.update(MSP_PODCAST_EMOTIONS[x] for x in batch[\"emotion_ix\"])\n",
    "# for i, (label, count) in enumerate(counter.most_common()):\n",
    "#     print(f\"{i+1}) {label:<15}\\t{count}\\t({count / counter.total():.1%})\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7a271a2-81e0-45ee-97cd-2e585fa55d65",
   "metadata": {},
   "source": [
    "1) neutral        \t31101\t(36.2%)\n",
    "2) no_agreement   \t17460\t(20.3%)\n",
    "3) happy          \t17165\t(20.0%)\n",
    "4) sad            \t5714\t(6.6%)\n",
    "5) angry          \t4634\t(5.4%)\n",
    "6) surprise       \t2981\t(3.5%)\n",
    "7) contempt       \t2442\t(2.8%)\n",
    "8) other          \t1848\t(2.1%)\n",
    "9) disgust        \t1460\t(1.7%)\n",
    "10) fear           \t1211\t(1.4%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a21717a1-8461-43e6-92cc-3be314d5ee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_datum(datum=None, batch=None, batch_ix=None):\n",
    "    if datum is None:\n",
    "        assert batch is not None and batch_ix is not None\n",
    "        datum = {key: val[batch_ix] for key, val in batch.items()}\n",
    "    elif batch is None:\n",
    "        assert datum is not None\n",
    "\n",
    "    print(f'Key: {datum[\"key\"]}\\tURL: {datum[\"url\"]}')\n",
    "    maybe_print_keys = [\n",
    "        [\"speaker\", \"gender\"],\n",
    "        [\"transcript\"],\n",
    "        [\"valence\", \"activation\", \"arousal\", \"domination\"],\n",
    "        [\"emotion_ix\", \"emotion\"],\n",
    "    ]\n",
    "    for key_list in maybe_print_keys:\n",
    "        print_string = \"\"\n",
    "        for key in key_list:\n",
    "            if key in datum:\n",
    "                print_string += f\"{key}: {datum[key]}\\t\"\n",
    "        if print_string != \"\":\n",
    "            print(print_string)\n",
    "\n",
    "    wavs = []\n",
    "    for wav_key in filter(lambda x: x.startswith(\"wav\"), datum.keys()):\n",
    "        num_samples_key = (\n",
    "            \"num_samples\"\n",
    "            if wav_key == \"wav\"\n",
    "            else \"num_samples_\" + wav_key.removeprefix(\"wav_\")\n",
    "        )\n",
    "        num_samples = datum.get(num_samples_key, datum[wav_key].shape[0])\n",
    "        wav = datum[wav_key][:num_samples]\n",
    "        wavs.append((wav_key, wav))\n",
    "    for wav_key, wav in wavs:\n",
    "        fig, axes = plt.subplots(nrows=2, figsize=(12, 4))\n",
    "        fig.tight_layout()\n",
    "        axes[0].plot(wav)\n",
    "        axes[0].set_title(wav_key)\n",
    "        axes[1].specgram(wav, Fs=sample_rate)\n",
    "        ipd.display(ipd.Audio(wav, rate=sample_rate))\n",
    "        ipd.display(fig)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e9aa799-1a6c-4fd6-99e3-846c64857b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import Normalize, Trim, PolarityInversion\n",
    "from data import PitchShift, apply_augmentation, Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1572491-2254-4967-a6f5-779e92a4fc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 16_000\n",
    "max_duration = 10  # has to be int and such that 3000 / (max_duration / 30) is an int\n",
    "assert 3000 * (max_duration / 30 / 2) % 1 == 0\n",
    "batch_size = 64\n",
    "dataloader_workers = 20\n",
    "emotions = [\"neutral\", \"happy\", \"sad\", \"angry\", \"fear\", \"surprise\", \"disgust\"]\n",
    "ix_to_emotion = {i: emotion for i, emotion in enumerate(emotions)}\n",
    "msp_labels_wanted = [\"neutral\", \"happy\", \"sad\", \"angry\", \"fear\", \"surprise\", \"disgust\"]\n",
    "ravdess_labels_wanted = [\n",
    "    \"neutral\",\n",
    "    \"happy\",\n",
    "    \"sad\",\n",
    "    \"angry\",\n",
    "    \"fearful\",\n",
    "    \"surprised\",\n",
    "    \"disgust\",\n",
    "]\n",
    "iemocap_labels_wanted = [\n",
    "    \"neutral\",\n",
    "    \"happy\",\n",
    "    \"sad\",\n",
    "    \"angry\",\n",
    "    \"fearful\",\n",
    "    \"surprised\",\n",
    "    \"disgust\",\n",
    "]\n",
    "assert set(msp_labels_wanted).issubset(MSP_PODCAST_EMOTIONS)\n",
    "assert len(emotions) == len(msp_labels_wanted)\n",
    "assert set(ravdess_labels_wanted).issubset(RAVDESS_EMOTIONS)\n",
    "assert len(emotions) == len(ravdess_labels_wanted)\n",
    "assert set(iemocap_labels_wanted).issubset(IEMOCAP_EMOTIONS)\n",
    "assert len(emotions) == len(iemocap_labels_wanted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c33a096-57f9-40f4-a1ff-146745f42d19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# duration = 0\n",
    "# for batch in tqdm(train_dl):\n",
    "#     pass\n",
    "#     duration += batch[\"num_samples\"].sum() / sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32c7e711-4a3b-4bdf-b166-d1eb9c79a0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = wds.DataPipeline(\n",
    "    *[\n",
    "        SampledShards(\n",
    "            urls=train_urls,\n",
    "            nshards=min(\n",
    "                max(1, (len(train_urls) // dataloader_workers)) * dataloader_workers,\n",
    "                len(train_urls),\n",
    "            ),\n",
    "        ),\n",
    "        wds.split_by_worker,\n",
    "        wds.tarfile_to_samples(),\n",
    "        FilterLabels(\n",
    "            labels_wanted=msp_labels_wanted,\n",
    "            labels_to_ix=MSP_PODCAST_EMOTION_TO_IX,\n",
    "            label_key=\"emotion\",\n",
    "        ),\n",
    "        wds.shuffle(batch_size * 64),\n",
    "        wds.map(\n",
    "            decode(dataset=\"MSP_PODCAST\", ix_to_label=ix_to_emotion, decode_json=False)\n",
    "        ),\n",
    "        wds.map(\n",
    "            apply_augmentation(\n",
    "                augmentation=Compose([Normalize(p=1.0), Trim(p=1.0)]),\n",
    "                out_key=\"wav\",\n",
    "                sample_rate=sample_rate,\n",
    "            )\n",
    "        ),\n",
    "        wds.map(crop(crop_duration=max_duration, random=True, keys=[\"wav\"])),\n",
    "        # wds.map(\n",
    "        #     apply_augmentation(\n",
    "        #         augmentation=Compose([PitchShift(sample_rate=sample_rate, p=1.0, device='cuda:1')]),\n",
    "        #         out_key=\"wav\",\n",
    "        #         p=1.,\n",
    "        #     )\n",
    "        # ),\n",
    "    ]\n",
    ")\n",
    "train_ds_batched = train_ds.compose(\n",
    "    wds.batched(\n",
    "        batchsize=batch_size,\n",
    "        collation_fn=collate_and_featurize(\n",
    "            list_keys=[\"key\", \"url\", \"emotion\", \"transcript\"],\n",
    "            tensor_keys=[\"emotion_ix\"],\n",
    "            wav_keys=[\"wav\"],\n",
    "            feature_keys=[\"wav\"],\n",
    "            feature_extractor=WhisperFeatureExtractor,\n",
    "            feature_kwargs={\"chunk_length\": max_duration},\n",
    "        ),\n",
    "        partial=False,\n",
    "    )\n",
    ")\n",
    "train_dl = wds.WebLoader(\n",
    "    train_ds_batched,\n",
    "    batch_size=None,\n",
    "    num_workers=dataloader_workers,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    # worker_init_fn=worker_init_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74dd3394-3605-4ac4-88d2-a02974c0fae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import JupyterProfiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b9d958d-0f65-436a-83a5-d22409f65e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with JupyterProfiler():\n",
    "#     for i, batch in enumerate(train_ds_batched):\n",
    "#         if i == 1:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b681d225-2aa9-4190-9954-6de26410d56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "292it [00:13, 21.40it/s]Exception ignored from cffi callback <function SoundFile._init_virtual_io.<locals>.vio_tell at 0x7fce6cc2de40>Exception ignored from cffi callback <function SoundFile._init_virtual_io.<locals>.vio_tell at 0x7fce6cc31800>Exception ignored from cffi callback <function SoundFile._init_virtual_io.<locals>.vio_tell at 0x7fce6cc44400>:\n",
      ":\n",
      ":\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kd2939/dev/miniconda3/envs/emo/lib/python3.11/site-packages/soundfile.py\", line 1264, in vio_tell\n",
      "  File \"/home/kd2939/dev/miniconda3/envs/emo/lib/python3.11/site-packages/soundfile.py\", line 1264, in vio_tell\n",
      "Exception ignored from cffi callback <function SoundFile._init_virtual_io.<locals>.vio_tell at 0x7fce6cde7060>  File \"/home/kd2939/dev/miniconda3/envs/emo/lib/python3.11/site-packages/soundfile.py\", line 1264, in vio_tell\n",
      "        :\n",
      "    @_ffi.callback(\"sf_vio_tell\")@_ffi.callback(\"sf_vio_tell\")@_ffi.callback(\"sf_vio_tell\")Traceback (most recent call last):\n",
      "\n",
      "\n",
      "\n",
      "  File \"/home/kd2939/dev/miniconda3/envs/emo/lib/python3.11/site-packages/soundfile.py\", line 1264, in vio_tell\n",
      "\n",
      "\n",
      "\n",
      "KeyboardInterruptKeyboardInterruptKeyboardInterrupt:     : : \n",
      "@_ffi.callback(\"sf_vio_tell\")\n",
      "\n",
      "\n",
      "\n",
      "KeyboardInterrupt: \n",
      "\n",
      "Exception ignored from cffi callback <function SoundFile._init_virtual_io.<locals>.vio_tell at 0x7fce6cc2c540>:\n",
      "Traceback (most recent call last):\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n",
      "  File \"/home/kd2939/dev/miniconda3/envs/emo/lib/python3.11/site-packages/soundfile.py\", line 1264, in vio_tell\n",
      "    @_ffi.callback(\"sf_vio_tell\")\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "for batch in tqdm(train_dl):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef89e091-f7dd-43f8-89c9-bb0e7f0917fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_pipeline = [\n",
    "    wds.SimpleShardList(dev_urls),\n",
    "    wds.split_by_worker,\n",
    "    wds.tarfile_to_samples(),\n",
    "    FilterLabels(\n",
    "        labels_wanted=msp_labels_wanted,\n",
    "        labels_to_ix=MSP_PODCAST_EMOTION_TO_IX,\n",
    "        label_key=\"emotion\",\n",
    "    ),\n",
    "    wds.map(\n",
    "        decode(dataset=\"MSP_PODCAST\", ix_to_label=ix_to_emotion, decode_json=False)\n",
    "    ),\n",
    "    wds.map(\n",
    "        apply_augmentation(\n",
    "            augmentation=Compose([Normalize(p=1.0)]),\n",
    "            out_key=\"wav\",\n",
    "            sample_rate=sample_rate,\n",
    "        )\n",
    "    ),\n",
    "    wds.map(crop(crop_duration=max_duration, random=True, keys=[\"wav\"])),\n",
    "    wds.batched(\n",
    "        batchsize=batch_size,\n",
    "        collation_fn=collate_and_featurize(\n",
    "            list_keys=[\"key\", \"url\", \"emotion\"],\n",
    "            tensor_keys=[\"emotion_ix\"],\n",
    "            wav_keys=[\"wav\"],\n",
    "            feature_keys=[\"wav\"],\n",
    "            feature_extractor=WhisperFeatureExtractor,\n",
    "            feature_kwargs={\"chunk_length\": max_duration},\n",
    "        ),\n",
    "        partial=False,\n",
    "    ),\n",
    "    Cache(shuffle=False),\n",
    "]\n",
    "dev_dl = wds.WebLoader(\n",
    "    wds.DataPipeline(*dev_pipeline),\n",
    "    batch_size=None,\n",
    "    num_workers=8,\n",
    "    prefetch_factor=4,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94985a93-ea5a-447b-b952-fe401c23bdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_= next(iter(dev_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7039ea11-4475-48f0-b627-7803fed260c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data_duration = 0.\n",
    "# for i, batch in enumerate(tqdm(train_dl)):\n",
    "#     training_data_duration += batch['num_samples'].sum() / sample_rate\n",
    "# num_train_batches = i + 1\n",
    "# train_dl.length = num_train_batches\n",
    "# for i, batch in enumerate(tqdm(dev_dl)):\n",
    "#     pass\n",
    "# num_dev_batches = i + 1\n",
    "# dev_dl.length = num_dev_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11bfe7a3-13cb-4b62-9250-3ffe452af3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Total training data duration: {training_data_duration/3600:.2f} hrs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "795c312e-da53-46d4-835f-7cbdc59f7f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38b7b1fd-0415-4f7a-b77e-efeb7a1fcfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(\n",
    "    # \"openai/whisper-tiny.en\",\n",
    "    \"openai/whisper-small.en\",\n",
    "    # \"openai/whisper-medium.en\",\n",
    "    max_duration=max_duration,\n",
    "    projection_dim=256,\n",
    "    num_classes=len(emotions),\n",
    "    freeze_conv_pos=True,\n",
    ")\n",
    "model = model.cuda()\n",
    "cross_entropy_criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.97)\n",
    "grad_scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8bd36c23-c5bd-4834-ad69-c5274feaae9b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    dataloader: wds.WebLoader,\n",
    "    criterion,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    lr_scheduler: torch.optim.lr_scheduler.LRScheduler,\n",
    "    grad_scaler: torch.cuda.amp.GradScaler,\n",
    "    forward_pass_iters: int,\n",
    "    grad_accum_iters: int,\n",
    "    grad_norm: float,\n",
    "    num_epochs: int,\n",
    "    print_every: int,\n",
    "):\n",
    "    model.train().cuda()\n",
    "    epoch_losses = []\n",
    "    for epoch_ix in range(num_epochs):\n",
    "        total = None if dataloader.length == -1 else dataloader.length\n",
    "        with tqdm(total=total) as pbar:\n",
    "            tot_epoch_loss = torch.tensor(0.0).cuda()\n",
    "            for batch_idx, batch in enumerate(dataloader):\n",
    "\n",
    "                # FORWARD PASS\n",
    "                with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                    out = model(batch[\"feats\"].cuda(), batch[\"attention_mask\"].cuda())\n",
    "                    loss = criterion(out, batch[\"emotion_ix\"].cuda())\n",
    "\n",
    "                # BACKWARD PASS\n",
    "                grad_scaler.scale(loss).backward()\n",
    "\n",
    "                # GRADIENT UPDATE\n",
    "                if (batch_idx + 1) % grad_accum_iters == 0:\n",
    "                    if grad_norm is not None:\n",
    "                        grad_scaler.unscale_(optimizer)\n",
    "                        torch.nn.utils.clip_grad_norm_(\n",
    "                            model.parameters(), max_norm=grad_norm\n",
    "                        )\n",
    "                    grad_scaler.step(optimizer)\n",
    "                    grad_scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                # PBAR UPDATE\n",
    "                with torch.no_grad():\n",
    "                    tot_epoch_loss += loss\n",
    "                pbar.update(1)\n",
    "                if ((batch_idx + 1) % print_every) == 0:\n",
    "                    pbar.set_description(\n",
    "                        f\"train_loss: {tot_epoch_loss.cpu().item() / batch_idx:.2f}\"\n",
    "                    )\n",
    "            # FINAL PBAR UPDATE\n",
    "            tot_epoch_loss = tot_epoch_loss.cpu().item() / batch_idx\n",
    "            pbar.set_description(f\"train_loss: {tot_epoch_loss:.2f}\")\n",
    "        epoch_losses.append(tot_epoch_loss)\n",
    "\n",
    "        # LR SCHEDULER\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        if dataloader.length == -1:\n",
    "            dataloader.length = batch_idx + 1\n",
    "    return epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f87cdb16-d6a8-4a49-83fc-bc2dd107c6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(\n",
    "    model: nn.Module,\n",
    "    dataloader: wds.WebLoader,\n",
    "    criterion,\n",
    "    forward_pass_iters: int,\n",
    "    print_every: int,\n",
    "    target_names: list[str],\n",
    "):\n",
    "    model.eval().cuda()\n",
    "    total = None if dataloader.length == -1 else dataloader.length\n",
    "    with tqdm(total=total) as pbar:\n",
    "        tot_epoch_loss = torch.tensor(0.0).cuda()\n",
    "        preds = []\n",
    "        truth = []\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            truth.append(batch[\"emotion_ix\"])\n",
    "\n",
    "            # FORWARD PASS\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                with torch.no_grad():\n",
    "                    out = model(\n",
    "                        batch[\"feats\"].cuda(),\n",
    "                        batch[\"attention_mask\"].cuda(),\n",
    "                    )\n",
    "                    loss = criterion(out, batch[\"emotion_ix\"].cuda())\n",
    "            preds.append(out.argmax(-1).cpu())\n",
    "\n",
    "            # PBAR UPDATE\n",
    "            with torch.no_grad():\n",
    "                tot_epoch_loss += loss\n",
    "            pbar.update(1)\n",
    "            if ((batch_idx + 1) % print_every) == 0:\n",
    "                pbar.set_description(\n",
    "                    f\"dev_loss: {tot_epoch_loss.cpu().item() / batch_idx:.2f}\"\n",
    "                )\n",
    "        # FINAL PBAR UPDATE\n",
    "        tot_epoch_loss = tot_epoch_loss.cpu().item() / batch_idx\n",
    "        pbar.set_description(f\"dev_loss: {tot_epoch_loss:.2f}\")\n",
    "    preds = torch.cat(preds).numpy()\n",
    "    truth = torch.cat(truth).numpy()\n",
    "    print(\n",
    "        classification_report(\n",
    "            y_true=truth,\n",
    "            y_pred=preds,\n",
    "            labels=list(range(len(target_names))),\n",
    "            target_names=target_names,\n",
    "            zero_division=0.0,\n",
    "        )\n",
    "    )\n",
    "    if dataloader.length == -1:\n",
    "        dataloader.length = batch_idx + 1\n",
    "    return tot_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "846b2cd7-121b-4502-b623-ea7c0d0a31cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(\n",
    "    model: nn.Module,\n",
    "    train_dataloader: wds.WebLoader,\n",
    "    dev_dataloader: wds.WebLoader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    lr_scheduler,\n",
    "    grad_scaler: torch.cuda.amp.GradScaler,\n",
    "    forward_pass_iters: int,\n",
    "    grad_accum_iters: int,\n",
    "    grad_norm: float,\n",
    "    num_epochs: int,\n",
    "    print_every: int,\n",
    "    validate_every: int,\n",
    "    target_names: list[str],\n",
    "):\n",
    "    train_losses = []\n",
    "    dev_losses = []\n",
    "    for epoch_ix in range(num_epochs):\n",
    "        train_losses.append(\n",
    "            (\n",
    "                epoch_ix + 1,\n",
    "                train(\n",
    "                    model=model,\n",
    "                    dataloader=train_dataloader,\n",
    "                    criterion=cross_entropy_criterion,\n",
    "                    optimizer=optimizer,\n",
    "                    lr_scheduler=lr_scheduler,\n",
    "                    grad_scaler=grad_scaler,\n",
    "                    forward_pass_iters=forward_pass_iters,\n",
    "                    grad_accum_iters=grad_accum_iters,\n",
    "                    grad_norm=grad_norm,\n",
    "                    num_epochs=1,\n",
    "                    print_every=print_every,\n",
    "                )[0],\n",
    "            )\n",
    "        )\n",
    "        if (epoch_ix + 1) % validate_every == 0:\n",
    "            dev_losses.append(\n",
    "                (\n",
    "                    epoch_ix + 1,\n",
    "                    validate(\n",
    "                        model=model,\n",
    "                        dataloader=dev_dataloader,\n",
    "                        criterion=cross_entropy_criterion,\n",
    "                        forward_pass_iters=forward_pass_iters,\n",
    "                        print_every=print_every,\n",
    "                        target_names=target_names,\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "    return train_losses, dev_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c760733-f441-41ca-be61-c14d7fe92405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_loss: 1.13: : 326it [01:38,  3.32it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = train_and_validate(\n",
    "    model=model,\n",
    "    train_dataloader=train_dl,\n",
    "    dev_dataloader=dev_dl,\n",
    "    criterion=cross_entropy_criterion,\n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    grad_scaler=grad_scaler,\n",
    "    forward_pass_iters=1,\n",
    "    grad_accum_iters=1,\n",
    "    grad_norm=None,\n",
    "    num_epochs=10,\n",
    "    print_every=10,\n",
    "    validate_every=1,\n",
    "    target_names=emotions,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "089655c5-a2a9-4b1b-a04f-2fb56195b71a",
   "metadata": {},
   "source": [
    "batch_size = 64\n",
    "train_loss: 1.13: : 326it [01:38,  3.32it/s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f57c2c53-37bf-43c3-8960-2c4b36203fbf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_results, dev_results = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d018b970-0a2d-4545-98a4-9fcb4b286de3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
